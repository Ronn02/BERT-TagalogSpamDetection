{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af989b7a-c1d7-4938-b34a-3080c1a7b41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained Tagalog BERT model and tokenizer\n",
    "model_name = \"jcblaise/bert-tagalog-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Define a custom classifier on top of BERT\n",
    "class SpamClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(SpamClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(bert_model.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Load your spam/ham dataset (e.g., from a CSV file)\n",
    "data = pd.read_csv(\"dataset.csv\")  # Make sure your CSV file has \"text\" and \"label\" columns\n",
    "\n",
    "# Tokenize and encode your dataset\n",
    "encoded_data = tokenizer(list(data[\"text\"]), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "input_ids = encoded_data.input_ids\n",
    "attention_mask = encoded_data.attention_mask\n",
    "\n",
    "# Convert labels to PyTorch tensors\n",
    "labels = torch.tensor(data[\"label\"].values)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    input_ids, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_attention_mask, val_attention_mask, _, _ = train_test_split(\n",
    "    attention_mask, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create DataLoader for training and validation data\n",
    "train_dataset = TensorDataset(train_inputs, train_attention_mask, train_labels)\n",
    "val_dataset = TensorDataset(val_inputs, val_attention_mask, val_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Define your SpamClassifier model\n",
    "num_classes = 2  # 2 classes: spam and not spam\n",
    "spam_classifier = SpamClassifier(bert_model, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(spam_classifier.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    spam_classifier.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = spam_classifier(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "spam_classifier.eval()\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = spam_classifier(input_ids, attention_mask)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predicted_labels.extend(predicted.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(np.array(predicted_labels) == np.array(true_labels))\n",
    "print(\"Validation Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f7f1b-7c2c-4bd4-9337-59d98268e6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
